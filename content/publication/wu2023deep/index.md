---
# title: "An example conference paper"
title: "Deep Ensemble Robustness by Adaptive Sampling in Dropout-Based Simultaneous Training"
authors:
- Quanwei Wu
- Bo Huang
- Yi Wang
- Zhiwei Ke
- Da Luo

date: ""
doi: ""

# Schedule page publish date (NOT publication's date).
# publishDate: "2023-09-30T00:00:00Z"
publishDate: "2023-07-16T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: In *The 26th European Conference on Artificial Intelligence*
publication_short: In *ECAI2023*

abstract: Recent studies show that an ensemble of deep networks can have better adversarial robustness by increasing the learning diversity of base models to limit adversarial transferability. However, existing schemes mostly rely on a second-order method for gradient regularization which usually involves a heavy computation overhead. In this paper, we propose a simple yet effective method which eliminates the use of a second-order optimization and significantly reduces the computation complexity of regularized simultaneous training of deep ensemble networks. For the first time, we show analytically that stochastic regularization by the proposed approach can promote both model smoothness and feature diversity of representation learning in the deep space. We also show that the proposed method is able to achieve a better gain of certified robustness. This is due to the effect of a prioritized feature selection enabled by an adaptive and continuous sampling of neuron activation among the base networks. Experimental results show that our method can improve adversarial robustness signiÔ¨Åcantly comparing with the existing ensemble models on several image benchmark datasets. The ensemble performance can be further boosted by complementing the stochastic regularization approach with other defense paradigms such as adversarial training.

# Summary. An optional shortened abstract.
# summary: We encouraging ensemble diversity on learning high-level feature representations and gradient dispersion in simultaneous training of deep ensemble networks.


# links:
url_pdf: https://ecai2023.eu/acceptedpapers
# url_code: ''
# url_dataset: ''
# url_poster:  https://virtual.2021.aaai.org/paper_AAAI-7120.html
# url_project: ''
# url_slides: ''
# url_source: ''
# url_video: ''
---
---
