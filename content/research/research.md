---
widget: blank
headless: true

# ... Put Your Section Options Here (title etc.) ...
title: Research | Current
subtitle:
weight: 10  # section position on page
design:
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns: '1'
---
<!-- <div>
  <span>
  Nowadays, deep learning is playing a core function in various real-world applications. Recent studies found that deep learning systems are vulnerable to attacks by adversarial machine learning. For example, adversarial examples can cause malfunction of the system at inference time. It is a cat-and-mouse game between adversarial attacks and defense. Existing approaches of both sides are often ad-hoc and thus limited in more realistic situations. We are interested in studying explainable methods that can achieve a better trade-off between the utility and security/privacy requirements of deep learning. The following are research areas that we are currently working on.
  </span>
</div>
<br>
<div class="section-heading col-12 mb-3 text-center"><h1 class="mb-0">In RESEARCH</h1></div>
<br> -->
<!-- <div style="margin: auto;" >
  <img style="margin: auto; width: 100%;" src="research.jpg">
</div> -->


<div style="padding-top: 30px;">
  <hr>
  <span>
        Nowadays, deep learning is at the core of many real-world application systems. Recent studies have found that deep learning empowered systems can be easily compromised by adversarial machine learning (AML). Like in other security studies, it is a cat-and-mouse game between the adversarial attacks and defense for deep learning. Existing methods from both sides are often ad-hoc and thus limited by their power to adapt in more realistic scenarios. We are interested in studying explainable methods in order to understand the learning process and achieve a better trade-off between the utility and security/privacy requirements of system models. The following are some research areas that we are currently working on:
  </span>
  <br>
  <!-- <hr> -->
      <ul style="padding-top: 30px;">
        <li>
          <a href="#Adversarial-Attacks">Adversarial Attacks</a>
        </li>
        <li>
          <a href="#Adversarial-Detection">Adversarial Detection</a>
        </li>
        <li>
          <a href="#Model-Robustness">Model Robustness</a>
        </li>
        <li>
          <a href="#Privacy-Protection">Privacy Protection</a>
        </li>
        <li>
          <a href="#Causality-Inference">Causality Inference</a>
        </li>
        <li>
          <a href="#Anomaly-Detection">Anomaly Detection</a>
        </li>
      </ul>
</div>

<hr>  
<div style="display: block;">
  <h2 id="Adversarial-Attacks" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Adversarial Attacks
  </h2>
  <div>
    <div style="padding-left: 20px; width: 100%">
      <span>
        Deep neural networks (DNN) have made significant progresses in a wide domain of machine learning, including image classification, object detection,  automatic speech recognition, content-based image retrieval (CBIR), steganography, and more. DNN provides an end-to-end learning approach that automates feature extraction with higher and more abstract level representation from the raw input. Recently, deep learning is found to be vulnerable under adversarial attacks. In particular, malicious input can be generated by imposing adversarial perturbations on the original input. Such adversarial examples are designed to induce wrongful decisions of the system model and are often imperceptible to human eyes. Currently, we are working on a number of query-efficient black-box attacks on different DNN-based application systems. Figure 1 illustrates an example of adversarial attack on CBIR systems, where the top-K retrieval results are subverted by an adversarial example of the query image.
      </span>
    </div>
    <div style="width: 100%;">
      <div style="margin:auto; width: 95%; height:100%;">
        <img style="margin: auto;width:60%;padding-top:20px;" src="Adversarial-Attacks.jpg">
      </div>
      <div style="margin:auto;text-align:center;padding-top:10px;padding-bottom: 30px;">
        <span> Figure 1 An example of adversarial attack on CBIR systems.</span>
      </div>
    </div>
  </div>
</div>  

<hr>  
<div>
  <h2 id="Adversarial-Detection" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Adversarial Detection
  </h2>
  <div>
    <div style="padding-left: 20px; width: 100%">
      <span>
        Adversarial detection attempts to distinguish adversarial examples from the normal inputs. The goal is to maximize the chance of allowing only the legitimate input to the intended model. Adversarial detection methods may be classified into model-dependent and model-agnostic approaches depending on interacting with the intended model or not. The dependent schemes often leverage the underlying model properties or internal states to detect the adversarial class, e.g., by adding detection layers/subnetworks or changing the loss/activation function. The model-agnostic detectors are mainly built based on analyzing the input and/or output feature characteristics without requiring access to the intended model. Currently, we are working on model-agnostic methods for adversarial detection at different layers of DNN. For example, we apply random perturbations to the model input for multiple times and use the statistical patterns of relative changes in the model output for adversarial detection. Figure 2 shows the detection accuracy of this method which is particularly effective for detecting small adversarial perturbations.
      </span>
    </div>
    <div style="width: 100%;">
      <div style="margin:auto; width: 95%; height:100%;">
        <img style="margin: auto;width:60%;padding-top:20px;" src="detection.jpg">
      </div>
      <div style="margin:auto;text-align:center;padding-top:10px;padding-bottom: 30px;">
        <span>Figure 2 Adversarial detection accuracy of a model-agnostic method with respect to an increasing strength of adversarial perturbation.</span>
      </div>
    </div>
  </div>
</div>

<hr>  
<div>
  <h2 id="Model-Robustness" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Model Robustness
  </h2>
  <div>
    <div style="float:left; padding-left: 20px; width: 100%">
      <span>
        Adversarial defense aims at improving the model robustness against adversarial attacks. Many defense techniques have been proposed in recent years. Popular defense paradigms include randomization techniques and training with regularizations. However, many of them are later shown defeated by stronger iterative attacks or adaptive adversaries.  So far, adversarial training has been considered as a standard method for defending against adversarial examples by data augmentation. However, adversarial training requires to generate large volumes of adversarial examples during the training phase, which introduces a high computational complexity. In general, there is a trade-off between classification accuracy and adversarial robustness in many defense methods. We are currently working on more effective methods of adversarial training by reducing its cost and improving its generalizability.  Figure 3 demonstrates our method of improving model robustness by promoting diversified simultaneous training of deep ensembles in comparison with STOA.
      </span>
    </div>
    <div style="width: 100%;">
      <div style="margin:auto; width: 95%; height:100%;">
        <img style="margin: auto;width:60%;padding-top:20px;" src="Model-Robustness.jpg">
      </div>
      <div style="margin:auto;text-align:center;padding-top:10px;padding-bottom: 30px;">
        <span>Figure 3 Improving model robustness by promoting diversified learning of deep features in deep ensembles.</span>
      </div>
    </div>
  </div>
</div>
<hr>  

<div>
  <h2 id="Privacy-Protection" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Privacy Protection
  </h2>
  <div>
    <div style="float:left; padding-left: 20px; width: 100%">
      <span>
        The field of machine learning privacy security includes two aspects. One is to focus on privacy leakage and expose privacy problems in machine learning models, including membership inference, property inference, sample reconstruction, and model extraction. Another focuses on privacy protection issues and studies how to prevent machine learning models from leaking privacy, including differential privacy, model compression, selective sharing and etc. Our research interest is privacy security issues in deep learning, including training data privacy and model privacy. At present, we have a certain understanding of the privacy issues in collaborative learning and have in-depth research on the data privacy leak from gradient.
      </span>
      <ul>
        <!-- <li>Adversarial Machine Learning</li>
        <li>Federated Learning</li>
        <li>Generated Adversarial Network</li> -->
      </ul>
    </div>
    <div style="display: inline-block; width: 100%;">
      <div style="margin-left:3%; float:left; width: 95%; height:100%;">
        <img style="width:100%;" src="privacy.jpg">
    </div>
  </div>
</div>
<hr>  
  
<div>
  <h2 id="Causality-Inference" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Causality Inference
  </h2>
  <div>
    <div style="float:left; padding-left: 20px; width: 100%">
      <span>
        Causality is a generic relationship between an effect and the cause that gives rise to it. It is hard to define, and human often only know intuitively about causes and effects. When it comes to learning causality with data, researchers need to be aware of the differences between statistical associations and causations. It contains spurious and causal relationship in the statistical associations. The former makes deep model less effective, and the latter is significant for both classification and detection task. Causality Inference is an interpretable pattern for majority of applications，such as supervised learning，semi-supervised learning domain adaptation，reinforcement learning, disentanglement and so on. We are currently exploring disentanglement learning to find major factors which are meaningful for the downstream task in various data.
      </span>
    </div>
    <div style="display: inline-block; width: 100%;">
      <div style="margin-left:3%; float:left; width: 95%; height:95%;">
        <img style="width:100%;" src="causality.png">
      </div>
    </div>
  </div>
</div>
<hr>  

<div>
  <h2 id="Anomaly-Detection" style="display: block; text-align: center; margin: 50px 0 20px 0;">
    Anomaly Detection
  </h2>
  <div>
    <div style="display: inline-block; width: 100%;">
      <div style="float:left; padding-left: 20px; width: 45%">
        <span>
          Anomaly detection is a step in data mining that identifies data points, events, and/or observations that deviate from a dataset’s normal behavior. Anomalous data can indicate critical incidents, such as a technical glitch, or potential opportunities, for instance a change in consumer behavior. Therefore, anomaly detection is an important problem that has been researched within diverse research areas and application domains. We are currently researching and developing a more general anomaly detection approach to provide a stronger guarantee for real application.
        </span>
      </div>
      <div style="margin-left:3%; float:left; width: 45%; height:95%;">
        <img style="width:100%;" src="Anomaly-Detection.jpg">
        <div style="margin:auto;text-align:center;padding-top:10px;padding-bottom: 30px;">
          <span>Figure 6 Adversarial detection accuracy of a model-agnostic method.</span>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- <div style="
    margin: auto;
    width: 95%;
    padding: 20px 20px 20px 40px;
    box-shadow: 0 4px 10px 0 rgb(0 0 0 / 20%);
">
  <hr style="height: 3px;">
  <br>
  <div style="margin-left: 20px;">
    <h2> WQW </h2>
    <ul style="margin: 20px;">
      <li>Adversarial Machine Learning</li>
      <li>Federated Learning</li>
      <li>Generated Adversarial Network</li>
    </ul>
  </div>
  <br>
</div> -->

